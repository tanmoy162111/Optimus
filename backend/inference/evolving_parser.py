"""
Evolving Parser System - Self-Improving Pattern Learning

This module extends the existing self-learning parser with true evolution capabilities:

1. ERROR PATTERN EVOLUTION:
   - Learns to recognize tool errors (command not found, permission denied, etc.)
   - Automatically skips parsing when error patterns are detected
   - Stores error signatures per tool for instant recognition

2. OUTPUT STRUCTURE EVOLUTION:
   - Uses LLM to analyze unknown output formats
   - Generates regex patterns automatically from successful LLM parsing
   - Validates and stores working patterns for future use

3. PATTERN REFINEMENT:
   - Tracks pattern success/failure rates
   - Automatically refines patterns based on feedback
   - Prunes ineffective patterns over time

4. CROSS-TOOL LEARNING:
   - Shares patterns between similar tools
   - Learns output format families (JSON, table, list, etc.)

Integration: 
    Replaces self_learning_parser.py as the main parsing entry point
    Uses existing parser_pattern_db.py for storage
    Uses existing ollama_client.py for LLM parsing
    Uses existing enhanced_output_parser.py as fallback
"""

import re
import json
import uuid
import hashlib
import logging
import sqlite3
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Set
from datetime import datetime
from enum import Enum
from dataclasses import dataclass, field, asdict
from contextlib import contextmanager

# Import existing components
from .enhanced_output_parser import EnhancedOutputParser
from .ollama_client import get_ollama_client, OllamaClient
from .parser_pattern_db import get_pattern_db, ParserPatternDB, ParsePattern

logger = logging.getLogger(__name__)


class ParseMethod(Enum):
    """Parsing method used - extended from self_learning_parser"""
    LEARNED = "learned"
    STRUCTURED = "structured"
    TOOL_SPECIFIC = "tool_specific"
    LLM = "llm"
    EVOLVED = "evolved"  # NEW: Pattern generated by evolution
    PATTERN = "pattern"
    HEURISTIC = "heuristic"
    ERROR_DETECTED = "error_detected"  # NEW: Error pattern matched
    FAILED = "failed"


@dataclass
class ErrorSignature:
    """Represents a learned error pattern"""
    pattern: str  # Regex pattern
    error_type: str  # command_not_found, permission_denied, etc.
    tool: Optional[str] = None  # Specific tool or None for universal
    count: int = 1
    last_seen: str = ""
    

@dataclass
class EvolvedPattern:
    """Represents an evolved parsing pattern"""
    tool: str
    pattern_type: str  # vuln_extractor, host_extractor, service_extractor
    regex: str
    field_mapping: Dict[str, int]  # field_name -> group_index
    confidence: float = 0.5
    success_count: int = 0
    failure_count: int = 0
    generated_by: str = "llm"  # llm, manual, cross_tool
    sample_match: str = ""  # Example of what this pattern matches
    created_at: str = ""
    

class EvolutionDatabase:
    """
    SQLite database for storing evolved patterns and error signatures.
    Extends the existing parser_patterns.db schema.
    """
    
    def __init__(self, db_path: str = None):
        if db_path is None:
            db_path = Path(__file__).parent.parent / 'data' / 'parser_evolution.db'
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_db()
        logger.info(f"[EvolutionDB] Initialized at {self.db_path}")
    
    @contextmanager
    def _get_connection(self):
        conn = sqlite3.connect(self.db_path, timeout=10)
        conn.row_factory = sqlite3.Row
        try:
            yield conn
            conn.commit()
        except Exception as e:
            conn.rollback()
            raise e
        finally:
            conn.close()
    
    def _init_db(self):
        """Initialize database schema"""
        with self._get_connection() as conn:
            # Error signatures table
            conn.execute('''
                CREATE TABLE IF NOT EXISTS error_signatures (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    pattern TEXT NOT NULL,
                    error_type TEXT NOT NULL,
                    tool TEXT,
                    count INTEGER DEFAULT 1,
                    last_seen TEXT,
                    created_at TEXT,
                    UNIQUE(pattern, tool)
                )
            ''')
            
            # Evolved patterns table
            conn.execute('''
                CREATE TABLE IF NOT EXISTS evolved_patterns (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    tool TEXT NOT NULL,
                    pattern_type TEXT NOT NULL,
                    regex TEXT NOT NULL,
                    field_mapping TEXT,
                    confidence REAL DEFAULT 0.5,
                    success_count INTEGER DEFAULT 0,
                    failure_count INTEGER DEFAULT 0,
                    generated_by TEXT DEFAULT 'llm',
                    sample_match TEXT,
                    created_at TEXT,
                    UNIQUE(tool, regex)
                )
            ''')
            
            # Pattern generation history
            conn.execute('''
                CREATE TABLE IF NOT EXISTS evolution_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    tool TEXT NOT NULL,
                    action TEXT NOT NULL,
                    details TEXT,
                    success INTEGER,
                    timestamp TEXT
                )
            ''')
            
            # Cross-tool pattern sharing
            conn.execute('''
                CREATE TABLE IF NOT EXISTS tool_families (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    tool TEXT NOT NULL UNIQUE,
                    family TEXT NOT NULL,
                    output_format TEXT
                )
            ''')
            
            # Initialize tool families
            tool_families = [
                ('nmap', 'network_scanner', 'structured'),
                ('masscan', 'network_scanner', 'structured'),
                ('nikto', 'web_scanner', 'line_based'),
                ('nuclei', 'web_scanner', 'json'),
                ('sqlmap', 'exploit_tool', 'mixed'),
                ('gobuster', 'directory_scanner', 'line_based'),
                ('ffuf', 'directory_scanner', 'json'),
                ('whatweb', 'fingerprinter', 'line_based'),
                ('wpscan', 'cms_scanner', 'mixed'),
            ]
            
            for tool, family, fmt in tool_families:
                conn.execute('''
                    INSERT OR IGNORE INTO tool_families (tool, family, output_format)
                    VALUES (?, ?, ?)
                ''', (tool, family, fmt))
            
            # Indexes
            conn.execute('CREATE INDEX IF NOT EXISTS idx_error_tool ON error_signatures(tool)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_evolved_tool ON evolved_patterns(tool)')
    
    # ============ Error Signature Methods ============
    
    def get_error_signatures(self, tool: Optional[str] = None) -> List[ErrorSignature]:
        """Get error signatures, optionally filtered by tool"""
        with self._get_connection() as conn:
            if tool:
                cursor = conn.execute(
                    'SELECT * FROM error_signatures WHERE tool = ? OR tool IS NULL ORDER BY count DESC',
                    (tool,)
                )
            else:
                cursor = conn.execute('SELECT * FROM error_signatures ORDER BY count DESC')
            
            return [
                ErrorSignature(
                    pattern=row['pattern'],
                    error_type=row['error_type'],
                    tool=row['tool'],
                    count=row['count'],
                    last_seen=row['last_seen'] or ''
                )
                for row in cursor
            ]
    
    def add_error_signature(self, pattern: str, error_type: str, tool: Optional[str] = None):
        """Add or update an error signature"""
        now = datetime.now().isoformat()
        
        with self._get_connection() as conn:
            try:
                conn.execute('''
                    INSERT INTO error_signatures (pattern, error_type, tool, count, last_seen, created_at)
                    VALUES (?, ?, ?, 1, ?, ?)
                ''', (pattern, error_type, tool, now, now))
            except sqlite3.IntegrityError:
                conn.execute('''
                    UPDATE error_signatures 
                    SET count = count + 1, last_seen = ?
                    WHERE pattern = ? AND (tool = ? OR (tool IS NULL AND ? IS NULL))
                ''', (now, pattern, tool, tool))
        
        logger.info(f"[EvolutionDB] Added error signature: {error_type} for {tool or 'universal'}")
    
    # ============ Evolved Pattern Methods ============
    
    def get_evolved_patterns(self, tool: str) -> List[EvolvedPattern]:
        """Get evolved patterns for a tool"""
        with self._get_connection() as conn:
            cursor = conn.execute('''
                SELECT * FROM evolved_patterns 
                WHERE tool = ? AND confidence >= 0.5
                ORDER BY success_count DESC, confidence DESC
            ''', (tool,))
            
            return [
                EvolvedPattern(
                    tool=row['tool'],
                    pattern_type=row['pattern_type'],
                    regex=row['regex'],
                    field_mapping=json.loads(row['field_mapping'] or '{}'),
                    confidence=row['confidence'],
                    success_count=row['success_count'],
                    failure_count=row['failure_count'],
                    generated_by=row['generated_by'],
                    sample_match=row['sample_match'] or '',
                    created_at=row['created_at'] or ''
                )
                for row in cursor
            ]
    
    def add_evolved_pattern(self, pattern: EvolvedPattern) -> bool:
        """Add a new evolved pattern"""
        now = datetime.now().isoformat()
        
        try:
            with self._get_connection() as conn:
                conn.execute('''
                    INSERT INTO evolved_patterns 
                    (tool, pattern_type, regex, field_mapping, confidence, 
                     generated_by, sample_match, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    pattern.tool, pattern.pattern_type, pattern.regex,
                    json.dumps(pattern.field_mapping), pattern.confidence,
                    pattern.generated_by, pattern.sample_match, now
                ))
            
            self._log_evolution(pattern.tool, 'pattern_added', 
                               f"Type: {pattern.pattern_type}, By: {pattern.generated_by}")
            return True
            
        except sqlite3.IntegrityError:
            logger.debug(f"[EvolutionDB] Pattern already exists for {pattern.tool}")
            return False
    
    def update_pattern_stats(self, tool: str, regex: str, success: bool):
        """Update pattern success/failure counts"""
        with self._get_connection() as conn:
            if success:
                conn.execute('''
                    UPDATE evolved_patterns 
                    SET success_count = success_count + 1,
                        confidence = MIN(0.95, confidence + 0.02)
                    WHERE tool = ? AND regex = ?
                ''', (tool, regex))
            else:
                conn.execute('''
                    UPDATE evolved_patterns 
                    SET failure_count = failure_count + 1,
                        confidence = MAX(0.1, confidence - 0.05)
                    WHERE tool = ? AND regex = ?
                ''', (tool, regex))
    
    def get_tool_family(self, tool: str) -> Optional[str]:
        """Get the family a tool belongs to"""
        with self._get_connection() as conn:
            row = conn.execute(
                'SELECT family FROM tool_families WHERE tool = ?', (tool,)
            ).fetchone()
            return row['family'] if row else None
    
    def get_family_patterns(self, family: str) -> List[EvolvedPattern]:
        """Get patterns from all tools in a family"""
        with self._get_connection() as conn:
            cursor = conn.execute('''
                SELECT ep.* FROM evolved_patterns ep
                JOIN tool_families tf ON ep.tool = tf.tool
                WHERE tf.family = ? AND ep.confidence >= 0.6
                ORDER BY ep.success_count DESC
            ''', (family,))
            
            return [
                EvolvedPattern(
                    tool=row['tool'],
                    pattern_type=row['pattern_type'],
                    regex=row['regex'],
                    field_mapping=json.loads(row['field_mapping'] or '{}'),
                    confidence=row['confidence'],
                    success_count=row['success_count'],
                    failure_count=row['failure_count'],
                    generated_by=row['generated_by'],
                    sample_match=row['sample_match'] or '',
                    created_at=row['created_at'] or ''
                )
                for row in cursor
            ]
    
    def _log_evolution(self, tool: str, action: str, details: str, success: bool = True):
        """Log evolution action"""
        with self._get_connection() as conn:
            conn.execute('''
                INSERT INTO evolution_history (tool, action, details, success, timestamp)
                VALUES (?, ?, ?, ?, ?)
            ''', (tool, action, details, 1 if success else 0, datetime.now().isoformat()))
    
    def cleanup_ineffective(self, min_attempts: int = 10, min_success_rate: float = 0.3):
        """Remove patterns that have proven ineffective"""
        with self._get_connection() as conn:
            result = conn.execute('''
                DELETE FROM evolved_patterns 
                WHERE (success_count + failure_count) >= ?
                AND (success_count * 1.0 / (success_count + failure_count + 1)) < ?
            ''', (min_attempts, min_success_rate))
            
            deleted = result.rowcount
            if deleted > 0:
                logger.info(f"[EvolutionDB] Removed {deleted} ineffective patterns")
            return deleted
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get evolution statistics"""
        with self._get_connection() as conn:
            error_count = conn.execute('SELECT COUNT(*) FROM error_signatures').fetchone()[0]
            pattern_count = conn.execute('SELECT COUNT(*) FROM evolved_patterns').fetchone()[0]
            history_count = conn.execute('SELECT COUNT(*) FROM evolution_history').fetchone()[0]
            
            # Top evolved patterns
            top_patterns = conn.execute('''
                SELECT tool, COUNT(*) as count 
                FROM evolved_patterns 
                GROUP BY tool 
                ORDER BY count DESC 
                LIMIT 5
            ''').fetchall()
            
            return {
                'error_signatures': error_count,
                'evolved_patterns': pattern_count,
                'evolution_events': history_count,
                'patterns_by_tool': {row['tool']: row['count'] for row in top_patterns}
            }


class PatternEvolver:
    """
    Evolves parsing patterns using LLM and heuristics.
    """
    
    def __init__(self, ollama: Optional[OllamaClient] = None):
        self.ollama = ollama or get_ollama_client()
        
        # Common vulnerability patterns to seed evolution
        self.seed_patterns = {
            'vuln_generic': [
                r'(?i)(vulnerability|vuln|finding)[:\s]+(.+)',
                r'(?i)(critical|high|medium|low)[:\s]+(.+)',
                r'(?i)\[(\w+)\]\s+(.+)',
            ],
            'cve': [
                r'(CVE-\d{4}-\d+)',
                r'(?i)cve[:\s]*([\d-]+)',
            ],
            'url': [
                r'(https?://[^\s<>"\']+)',
                r'(?i)url[:\s]+([^\s]+)',
            ],
            'ip_port': [
                r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})[:\s]*(\d+)',
                r'(?i)port[:\s]*(\d+)',
            ],
        }
    
    def evolve_from_llm_output(self, tool: str, raw_output: str, 
                               llm_findings: List[Dict]) -> List[EvolvedPattern]:
        """
        Generate regex patterns from successful LLM parsing.
        
        Args:
            tool: Tool name
            raw_output: Original tool output
            llm_findings: Findings extracted by LLM
            
        Returns:
            List of evolved patterns
        """
        evolved = []
        
        for finding in llm_findings:
            evidence = finding.get('evidence', '')
            if not evidence or len(evidence) < 10:
                continue
            
            # Try to create a pattern that matches this evidence
            pattern = self._create_pattern_for_evidence(evidence, raw_output)
            if pattern:
                evolved.append(EvolvedPattern(
                    tool=tool,
                    pattern_type='vuln_extractor',
                    regex=pattern,
                    field_mapping={'evidence': 0, 'type': 1} if '(' in pattern else {},
                    confidence=0.6,
                    generated_by='llm_evolution',
                    sample_match=evidence[:100]
                ))
        
        return evolved
    
    def _create_pattern_for_evidence(self, evidence: str, full_output: str) -> Optional[str]:
        """Create a regex pattern that matches evidence in output"""
        if evidence not in full_output:
            return None
        
        try:
            # Find the line containing the evidence
            for line in full_output.split('\n'):
                if evidence in line:
                    # Create pattern from line structure
                    pattern = self._generalize_line_pattern(line, evidence)
                    if pattern and self._validate_pattern(pattern, full_output):
                        return pattern
            return None
        except Exception as e:
            logger.debug(f"[PatternEvolver] Pattern creation failed: {e}")
            return None
    
    def _generalize_line_pattern(self, line: str, evidence: str) -> Optional[str]:
        """Generalize a line into a regex pattern"""
        try:
            # Find evidence position
            start = line.find(evidence)
            if start == -1:
                return None
            
            # Get prefix and suffix
            prefix = line[:start]
            suffix = line[start + len(evidence):]
            
            # Escape and generalize prefix
            prefix_pattern = re.escape(prefix)
            prefix_pattern = re.sub(r'\\\s+', r'\\s+', prefix_pattern)
            prefix_pattern = re.sub(r'\\\d+', r'\\d+', prefix_pattern)
            
            # Create capture group for evidence
            # Make it flexible but specific enough
            evidence_pattern = r'(.+?)' if len(evidence) > 30 else r'([^\n]+?)'
            
            # Generalize suffix (just the beginning)
            suffix_start = suffix[:10] if len(suffix) > 10 else suffix
            suffix_pattern = re.escape(suffix_start)
            suffix_pattern = re.sub(r'\\\s+', r'\\s*', suffix_pattern)
            
            full_pattern = prefix_pattern + evidence_pattern + suffix_pattern
            
            # Validate
            compiled = re.compile(full_pattern)
            return full_pattern
            
        except re.error:
            return None
    
    def _validate_pattern(self, pattern: str, output: str) -> bool:
        """Validate that pattern works on the output"""
        try:
            compiled = re.compile(pattern, re.MULTILINE | re.IGNORECASE)
            matches = compiled.findall(output)
            # Good pattern should match but not too many times (avoid over-matching)
            return 1 <= len(matches) <= 50
        except:
            return False
    
    def generate_pattern_with_llm(self, tool: str, sample_output: str) -> List[EvolvedPattern]:
        """
        Use LLM to generate parsing patterns for unknown output.
        
        OPTIMIZED FOR CODELLAMA 7B:
        - Simpler prompts
        - Single-task focus
        - Explicit format requirements
        """
        if not self.ollama or not self.ollama.is_available():
            return []
        
        # CodeLlama 7B works better with simpler, focused prompts
        prompt = f"""Extract vulnerability patterns from this {tool} output.

OUTPUT:
{sample_output[:800]}

Write a Python regex pattern to find vulnerabilities. Just the pattern, nothing else.
Example format: r'VULNERABILITY: (.+)'

Pattern:"""
        
        try:
            response = self.ollama.generate(prompt, temperature=0.1, max_tokens=100)
            if not response:
                return []
            
            # Clean response - CodeLlama often adds extra text
            response = response.strip()
            
            # Extract just the regex pattern
            pattern = None
            
            # Try to find r'...' or "..." pattern
            regex_match = re.search(r"r?['\"](.+?)['\"]", response)
            if regex_match:
                pattern = regex_match.group(1)
            elif response.startswith('r\'') or response.startswith('r"'):
                pattern = response[2:-1]
            elif not any(c in response for c in [' ', '\n']) and len(response) < 100:
                # Might be raw pattern
                pattern = response
            
            if not pattern:
                return []
            
            # Validate pattern
            try:
                compiled = re.compile(pattern, re.IGNORECASE)
                matches = compiled.findall(sample_output)
                if 0 < len(matches) <= 50:
                    return [EvolvedPattern(
                        tool=tool,
                        pattern_type='vuln_extractor',
                        regex=pattern,
                        field_mapping={},
                        confidence=0.4,  # Lower confidence for 7B model
                        generated_by='llm_7b',
                        sample_match=str(matches[0])[:100] if matches else ''
                    )]
            except re.error:
                pass
            
            return []
            
        except Exception as e:
            logger.debug(f"[PatternEvolver] LLM pattern generation failed: {e}")
            return []


class EvolvingParser:
    """
    Self-evolving parser that learns from errors and successes.
    
    This is the main entry point that replaces SelfLearningParser.
    It adds evolution capabilities while maintaining backward compatibility.
    """
    
    def __init__(self, enable_llm: bool = True, enable_evolution: bool = True):
        """
        Initialize the evolving parser.
        
        Args:
            enable_llm: Whether to use Ollama LLM
            enable_evolution: Whether to enable pattern evolution
        """
        # Core components
        self.base_parser = EnhancedOutputParser()
        self.pattern_db = get_pattern_db()
        self.ollama = get_ollama_client() if enable_llm else None
        
        # Evolution components
        self.enable_evolution = enable_evolution
        if enable_evolution:
            self.evolution_db = EvolutionDatabase()
            self.evolver = PatternEvolver(self.ollama)
        else:
            self.evolution_db = None
            self.evolver = None
        
        # Load error signatures into memory for fast matching
        self._error_patterns: List[Tuple[re.Pattern, str]] = []
        self._load_error_patterns()
        
        # Statistics
        self.stats = {
            'total': 0,
            'learned': 0,
            'evolved': 0,
            'llm': 0,
            'base_parser': 0,
            'error_detected': 0,
            'failed': 0,
            'patterns_evolved': 0,
            'errors_learned': 0,
        }
        
        logger.info(f"[EvolvingParser] Initialized (LLM: {enable_llm}, Evolution: {enable_evolution})")
    
    def _load_error_patterns(self):
        """Load error patterns into memory"""
        # Built-in error patterns
        builtin_errors = [
            (r'command not found', 'command_not_found'),
            (r'not found', 'not_found'),
            (r'\[TOOL_NOT_FOUND\]', 'tool_not_found'),
            (r'No such file or directory', 'file_not_found'),
            (r'Permission denied', 'permission_denied'),
            (r'cannot execute', 'cannot_execute'),
            (r'is not recognized', 'not_recognized'),
            (r'Unable to locate', 'unable_to_locate'),
            (r'Connection refused', 'connection_refused'),
            (r'Connection timed out', 'connection_timeout'),
            (r'Name or service not known', 'dns_error'),
            (r'No route to host', 'no_route'),
        ]
        
        for pattern, error_type in builtin_errors:
            try:
                self._error_patterns.append((re.compile(pattern, re.IGNORECASE), error_type))
            except re.error:
                pass
        
        # Load from database
        if self.evolution_db:
            for sig in self.evolution_db.get_error_signatures():
                try:
                    compiled = re.compile(sig.pattern, re.IGNORECASE)
                    self._error_patterns.append((compiled, sig.error_type))
                except re.error:
                    pass
    
    def parse(self, tool_name: str, stdout: str, stderr: str = "",
              command: str = "", target: str = "") -> Dict[str, Any]:
        """
        Main parsing entry point with evolution capabilities.
        
        Flow:
        1. Check for error patterns (skip parsing if error detected)
        2. Try evolved patterns first
        3. Try learned patterns (from pattern_db)
        4. Try base parser
        5. Try LLM parsing
        6. Evolve new patterns from success
        7. Learn from failures
        """
        self.stats['total'] += 1
        
        tool_lower = tool_name.lower().replace('.sh', '').replace('.py', '').strip()
        combined_output = stdout + ('\n' + stderr if stderr else '')
        
        context = {
            'tool': tool_lower,
            'command': command,
            'target': target,
        }
        
        # ===== STEP 1: Check for errors =====
        error_result = self._check_for_errors(combined_output, tool_lower)
        if error_result:
            self.stats['error_detected'] += 1
            return error_result
        
        # Skip empty outputs
        if not combined_output or len(combined_output.strip()) < 5:
            return self._empty_result(stdout, ParseMethod.FAILED, "Empty output")
        
        # ===== STEP 2: Try evolved patterns =====
        if self.enable_evolution and self.evolution_db:
            result = self._try_evolved_patterns(tool_lower, combined_output, context)
            if result and self._has_findings(result):
                self.stats['evolved'] += 1
                return result
        
        # ===== STEP 3: Try learned patterns =====
        if self.pattern_db:
            pattern = self.pattern_db.find_pattern(tool_lower, combined_output)
            if pattern and pattern.success_count >= 3:
                result = self._apply_learned_pattern(pattern, combined_output, context)
                if result and self._has_findings(result):
                    self.stats['learned'] += 1
                    result['parse_method'] = ParseMethod.LEARNED.value
                    return result
        
        # ===== STEP 4: Try base parser =====
        try:
            result = self.base_parser.parse(tool_lower, stdout, stderr, command, target)
            if self._has_findings(result):
                self.stats['base_parser'] += 1
                # Learn from success
                self._learn_from_success(tool_lower, combined_output, result)
                return result
            base_result = result
        except Exception as e:
            logger.debug(f"[EvolvingParser] Base parser error: {e}")
            base_result = None
        
        # ===== STEP 5: Try LLM parsing =====
        if self.ollama and self.ollama.is_available():
            try:
                llm_result = self.ollama.parse_tool_output(
                    tool_lower, combined_output, target, context
                )
                
                if llm_result and self._has_findings(llm_result):
                    self.stats['llm'] += 1
                    normalized = self._normalize_llm_result(llm_result, tool_lower, context)
                    
                    # EVOLVE: Generate patterns from LLM success
                    if self.enable_evolution:
                        self._evolve_from_llm_success(tool_lower, combined_output, normalized)
                    
                    normalized['parse_method'] = ParseMethod.LLM.value
                    normalized['raw_output'] = stdout
                    return normalized
                    
            except Exception as e:
                logger.debug(f"[EvolvingParser] LLM parsing error: {e}")
        
        # ===== STEP 6: Return base result or fail =====
        if base_result and self._has_findings(base_result):
            return base_result
        
        self.stats['failed'] += 1
        
        # EVOLVE: Try to generate patterns for this unknown format
        if self.enable_evolution:
            self._try_evolve_for_unknown(tool_lower, combined_output)
        
        return self._empty_result(stdout, ParseMethod.FAILED, "No findings extracted")
    
    def _check_for_errors(self, output: str, tool: str) -> Optional[Dict]:
        """Check if output contains error patterns"""
        output_lower = output.lower()
        
        for pattern, error_type in self._error_patterns:
            if pattern.search(output_lower):
                logger.debug(f"[EvolvingParser] Error detected for {tool}: {error_type}")
                
                # Learn this error pattern if new
                if self.enable_evolution:
                    self._learn_error_pattern(output, error_type, tool)
                
                return {
                    'vulnerabilities': [],
                    'hosts': [],
                    'services': [],
                    'raw_output': output,
                    'parse_method': ParseMethod.ERROR_DETECTED.value,
                    'parse_confidence': 1.0,
                    'error_type': error_type,
                    'tool_error': True
                }
        
        return None
    
    def _learn_error_pattern(self, output: str, error_type: str, tool: str):
        """Learn a new error pattern from output"""
        if not self.evolution_db:
            return
        
        # Find the specific line with the error
        for line in output.split('\n'):
            line = line.strip()
            if not line or len(line) < 10:
                continue
            
            # Check if this line matches any known error type
            for pattern, etype in self._error_patterns:
                if pattern.search(line):
                    # Create a more specific pattern from this line
                    new_pattern = self._create_error_pattern(line)
                    if new_pattern:
                        self.evolution_db.add_error_signature(new_pattern, error_type, tool)
                        self.stats['errors_learned'] += 1
                    break
    
    def _create_error_pattern(self, line: str) -> Optional[str]:
        """Create an error pattern from a line"""
        try:
            # Escape and generalize
            pattern = re.escape(line[:50])  # First 50 chars
            pattern = re.sub(r'\\\s+', r'\\s+', pattern)
            pattern = re.sub(r'\\\d+', r'\\d*', pattern)
            
            # Verify it compiles
            re.compile(pattern)
            return pattern
        except:
            return None
    
    def _try_evolved_patterns(self, tool: str, output: str, 
                              context: Dict) -> Optional[Dict]:
        """Try to parse using evolved patterns"""
        if not self.evolution_db:
            return None
        
        vulnerabilities = []
        patterns_used = []
        
        # Get evolved patterns for this tool
        evolved = self.evolution_db.get_evolved_patterns(tool)
        
        # Also try patterns from tool family
        family = self.evolution_db.get_tool_family(tool)
        if family:
            family_patterns = self.evolution_db.get_family_patterns(family)
            evolved.extend(family_patterns)
        
        for pattern in evolved:
            try:
                compiled = re.compile(pattern.regex, re.MULTILINE | re.IGNORECASE)
                matches = compiled.findall(output)
                
                for match in matches[:20]:  # Limit matches
                    evidence = match if isinstance(match, str) else match[0] if match else ''
                    if evidence and len(evidence) >= 10:
                        vulnerabilities.append({
                            'id': str(uuid.uuid4()),
                            'type': pattern.pattern_type.replace('_extractor', ''),
                            'name': f"Finding from {tool}",
                            'severity': 5.0,
                            'confidence': pattern.confidence,
                            'location': context.get('target', 'Unknown'),
                            'evidence': evidence[:500],
                            'exploitable': False,
                            'tool': tool,
                        })
                        patterns_used.append(pattern.regex)
                
            except re.error:
                continue
        
        if vulnerabilities:
            # Update pattern success stats
            for regex in set(patterns_used):
                self.evolution_db.update_pattern_stats(tool, regex, True)
            
            return {
                'vulnerabilities': vulnerabilities,
                'hosts': [],
                'services': [],
                'raw_output': output,
                'parse_method': ParseMethod.EVOLVED.value,
                'parse_confidence': 0.7
            }
        
        return None
    
    def _apply_learned_pattern(self, pattern: ParsePattern, output: str,
                               context: Dict) -> Optional[Dict]:
        """Apply a learned pattern to extract findings"""
        vulnerabilities = []
        
        for regex in pattern.extraction_patterns:
            try:
                compiled = re.compile(regex, re.IGNORECASE | re.MULTILINE)
                for match in compiled.finditer(output):
                    matched_text = match.group(0)
                    vulnerabilities.append({
                        'id': str(uuid.uuid4()),
                        'type': pattern.field_mappings.get('type', 'finding'),
                        'name': matched_text[:100],
                        'severity': float(pattern.field_mappings.get('default_severity', 5.0)),
                        'confidence': min(0.85 + pattern.confidence_boost, 0.95),
                        'location': context.get('target', 'Unknown'),
                        'evidence': matched_text[:500],
                        'exploitable': pattern.field_mappings.get('exploitable', 'false') == 'true',
                        'tool': context.get('tool', 'unknown'),
                    })
            except re.error:
                continue
        
        if vulnerabilities:
            return {
                'vulnerabilities': vulnerabilities,
                'hosts': [],
                'services': [],
                'raw_output': output
            }
        return None
    
    def _learn_from_success(self, tool: str, output: str, result: Dict):
        """Learn from successful parsing"""
        if not self.pattern_db:
            return
        
        try:
            extraction_patterns = []
            field_mappings = {}
            
            for vuln in result.get('vulnerabilities', [])[:5]:
                evidence = vuln.get('evidence', '')
                if evidence and len(evidence) >= 15:
                    pattern = self._create_pattern_from_evidence(evidence)
                    if pattern:
                        extraction_patterns.append(pattern)
                
                if not field_mappings:
                    field_mappings = {
                        'type': vuln.get('type', 'finding'),
                        'default_severity': str(vuln.get('severity', 5.0)),
                        'exploitable': str(vuln.get('exploitable', False)).lower()
                    }
            
            if extraction_patterns:
                self.pattern_db.store_pattern(
                    tool=tool,
                    output=output,
                    extraction_patterns=extraction_patterns[:5],
                    field_mappings=field_mappings
                )
        except Exception as e:
            logger.debug(f"[EvolvingParser] Learning failed: {e}")
    
    def _create_pattern_from_evidence(self, evidence: str) -> Optional[str]:
        """Create regex pattern from evidence"""
        if len(evidence) < 15:
            return None
        
        try:
            sample = evidence[:80]
            pattern = re.escape(sample)
            pattern = re.sub(r'\\ +', r'\\s+', pattern)
            pattern = re.sub(r'\\\d+', r'\\d+', pattern)
            re.compile(pattern)
            return pattern
        except re.error:
            return None
    
    def _evolve_from_llm_success(self, tool: str, output: str, result: Dict):
        """Evolve patterns from successful LLM parsing"""
        if not self.evolver or not self.evolution_db:
            return
        
        try:
            findings = result.get('vulnerabilities', [])
            evolved = self.evolver.evolve_from_llm_output(tool, output, findings)
            
            for pattern in evolved:
                if self.evolution_db.add_evolved_pattern(pattern):
                    self.stats['patterns_evolved'] += 1
                    logger.info(f"[EvolvingParser] Evolved new pattern for {tool}")
                    
        except Exception as e:
            logger.debug(f"[EvolvingParser] Evolution failed: {e}")
    
    def _try_evolve_for_unknown(self, tool: str, output: str):
        """Try to evolve patterns for unknown output format"""
        if not self.evolver or not self.evolution_db:
            return
        
        try:
            # Only attempt if output looks like it might have findings
            if any(kw in output.lower() for kw in ['vuln', 'finding', 'critical', 'high', 'cve']):
                patterns = self.evolver.generate_pattern_with_llm(tool, output)
                for pattern in patterns:
                    if self.evolution_db.add_evolved_pattern(pattern):
                        self.stats['patterns_evolved'] += 1
        except Exception as e:
            logger.debug(f"[EvolvingParser] Unknown format evolution failed: {e}")
    
    def _normalize_llm_result(self, llm_result: Dict, tool: str, context: Dict) -> Dict:
        """Normalize LLM result to standard format"""
        vulnerabilities = []
        
        for vuln in llm_result.get('vulnerabilities', []):
            normalized = {
                'id': str(uuid.uuid4()),
                'type': str(vuln.get('type', 'unknown')).lower().replace(' ', '_'),
                'name': str(vuln.get('name', 'Unknown Finding'))[:200],
                'severity': self._normalize_severity(vuln.get('severity', 5.0)),
                'confidence': vuln.get('confidence', 0.75),
                'location': str(vuln.get('location', context.get('target', 'Unknown')))[:500],
                'evidence': str(vuln.get('evidence', ''))[:1000],
                'exploitable': bool(vuln.get('exploitable', False)),
                'tool': tool,
            }
            if vuln.get('cve'):
                normalized['cve'] = vuln['cve']
            vulnerabilities.append(normalized)
        
        return {
            'vulnerabilities': vulnerabilities,
            'hosts': llm_result.get('hosts', []),
            'services': llm_result.get('services', [])
        }
    
    def _normalize_severity(self, value: Any) -> float:
        """Normalize severity to 0-10 scale"""
        if isinstance(value, (int, float)):
            return max(0.0, min(10.0, float(value)))
        if isinstance(value, str):
            severity_map = {
                'critical': 9.5, 'high': 7.5, 'medium': 5.0,
                'moderate': 5.0, 'low': 2.5, 'info': 1.0
            }
            return severity_map.get(value.lower().strip(), 5.0)
        return 5.0
    
    def _has_findings(self, result: Optional[Dict]) -> bool:
        """Check if result has findings"""
        if not result:
            return False
        return bool(result.get('vulnerabilities') or 
                   result.get('hosts') or 
                   result.get('services'))
    
    def _empty_result(self, raw_output: str, method: ParseMethod, note: str = "") -> Dict:
        """Create empty result"""
        return {
            'vulnerabilities': [],
            'hosts': [],
            'services': [],
            'raw_output': raw_output,
            'parse_method': method.value,
            'parse_confidence': 0.0,
            'parse_note': note
        }
    
    # Compatibility methods
    def parse_tool_output(self, tool_name: str, stdout: str, stderr: str = "") -> Dict:
        """Backward-compatible interface"""
        return self.parse(tool_name, stdout, stderr)
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get parser statistics"""
        total = max(self.stats['total'], 1)
        
        stats = {
            **self.stats,
            'success_rate': (total - self.stats['failed']) / total,
            'evolution_rate': self.stats['patterns_evolved'] / total if total > 0 else 0,
        }
        
        if self.evolution_db:
            stats['evolution_db'] = self.evolution_db.get_statistics()
        if self.pattern_db:
            stats['pattern_db'] = self.pattern_db.get_statistics()
        
        return stats
    
    def cleanup(self):
        """Cleanup ineffective patterns"""
        deleted = 0
        if self.evolution_db:
            deleted += self.evolution_db.cleanup_ineffective()
        if self.pattern_db:
            deleted += self.pattern_db.cleanup_ineffective_patterns()
        return deleted


# Singleton instance
_evolving_parser: Optional[EvolvingParser] = None


def get_evolving_parser(enable_llm: bool = True, 
                        enable_evolution: bool = True) -> EvolvingParser:
    """Get or create singleton evolving parser"""
    global _evolving_parser
    if _evolving_parser is None:
        _evolving_parser = EvolvingParser(enable_llm=enable_llm, 
                                          enable_evolution=enable_evolution)
    return _evolving_parser
